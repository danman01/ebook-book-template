# Apache Spark Cheat Sheet
## Transformations (return new RDD's – Lazy)

Where                                                                                                               | Function                                                                                                                                                                                                                         | DStream API                                                                                                                                                                                       | Description
------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [map(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#map%28scala.Function1, scala.reflect.ClassTag%29)                                                                                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#map%28scala.Function1, scala.reflect.ClassTag%29)                                             | Return a new distributed dataset formed by passing each element of the source through a function func.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [filter(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#filter%28scala.Function1%29)                                                                                                      | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#filter%28scala.Function1%29)                                                                  | Return a new dataset formed by selecting those elements of the source on which function returns true.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [filterByRange(lower, upper)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html#filterByRange%28K, K%29)                                                                               | No                                                                                                                                                                                                | Returns an RDD containing only the elements in the the inclusive range lower to upper.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [flatMap(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#flatMap%28scala.Function1, scala.reflect.ClassTag%29)                                                                            | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#flatMap%28scala.Function1, scala.reflect.ClassTag%29)                                         | Similar to map, but each input item can be mapped to 0 or more output items (so function should return a Seq rather than a single item).
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [mapPartitions(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#mapPartitions%28scala.Function1, boolean, scala.reflect.ClassTag%29)                                                       | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#mapPartitions%28scala.Function1, boolean, scala.reflect.ClassTag%29)                          | Similar to map, but runs separately on each partition (block) of the RDD, so function must be of type Iterator<T> => Iterator<U> when running on an RDD of type T.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [mapPartitionsWithIndex(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#mapPartitionsWithIndex%28scala.Function2, boolean, scala.reflect.ClassTag%29)                                     | No                                                                                                                                                                                                | Similar to mapPartitions, but also provides function with an integer value representing the index of the partition, so function must be of type (Int, Iterator<T>) => Iterator<U> when running on an RDD of type T.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [sample(withReplacement, fraction, seed)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#sample%28boolean, double, long%29)                                                                         | No                                                                                                                                                                                                | Sample a fraction fraction of the data, with or without replacement, using a given random number generator seed.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [union(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#union%28org.apache.spark.rdd.RDD%29)                                                                                           | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#union%28org.apache.spark.streaming.dstream.DStream%29)                                        | Return a new dataset that contains the union of the elements in the source dataset and the argument.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [intersection(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#intersection%28org.apache.spark.rdd.RDD%29)                                                                             | No                                                                                                                                                                                                | Return a new RDD that contains the intersection of elements in the source dataset and the argument.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [distinct([numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#distinct%28%29)                                                                                                               | No                                                                                                                                                                                                | Return a new dataset that contains the distinct elements of the source dataset.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [groupByKey([numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#groupByKey%28%29)                                                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#groupByKey%28%29)                                                                | When called on a dataset of (K, V) pairs, returns a dataset of (K, Iterable<V>) pairs. Note: If you are grouping in order to perform an aggregation (such as a sum or average) over each key, using reduceByKey or aggregateByKey will yield much better performance. Note: By default, the level of parallelism in the output depends on the number of partitions of the parent RDD. You can pass an optional numTasks argument to set a different number of tasks.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [reduceByKey(func, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#reduceByKey%28scala.Function2%29)                                                                       | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKey%28scala.Function2%29)                                                | When called on a dataset of (K, V) pairs, returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function func, which must be of type (V,V) => V. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                    | [aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#aggregateByKey%28U, scala.Function2, scala.Function2, scala.reflect.ClassTag%29) | No                                                                                                                                                                                                | When called on a dataset of (K, V) pairs, returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type, while avoiding unnecessary allocations. Like in groupByKey, the number of reduce tasks is configurable through an optional second argument.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [sortByKey([ascending], [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html)                                                                                                | No                                                                                                                                                                                                | When called on a dataset of (K, V) pairs where K implements Ordered, returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                    | [join(otherDataset, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#join%28org.apache.spark.rdd.RDD%29)                                                                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#join%28org.apache.spark.streaming.dstream.DStream, scala.reflect.ClassTag%29)    | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [cogroup(otherDataset, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#cogroup%28org.apache.spark.rdd.RDD%29)                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#cogroup%28org.apache.spark.streaming.dstream.DStream, scala.reflect.ClassTag%29) | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable<V>, Iterable<W>)) tuples. This operation is also called groupWith.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [cartesian(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#cartesian%28org.apache.spark.rdd.RDD, scala.reflect.ClassTag%29)                                                           | No                                                                                                                                                                                                | When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [pipe(command, [envVars])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#pipe%28scala.collection.Seq, scala.collection.Map, scala.Function1, scala.Function2, boolean%29)                          | No                                                                                                                                                                                                | Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script. RDD elements are written to the process's stdin and lines output to its stdout are returned as an RDD of strings.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [coalesce(numPartitions)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#coalesce%28int, boolean, scala.math.Ordering%29)                                                                           | No                                                                                                                                                                                                | Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                        | [repartition(numPartitions)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#repartition%28int, scala.math.Ordering%29)                                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#repartition%28int%29)                                                                         | Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [repartitionAndSortWithinPartitions(partitioner)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html)                                                                                   | No                                                                                                                                                                                                | Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. This is more efficient than calling repartition and then sorting within each partition because it can push the sorting down into the shuffle machinery.

## Actions (return values – NOT Lazy)

Where                                                                                                         | Function                                                                                                                                                      | DStream API                                                                                                                                                    | Description
------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [reduce(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#reduce%28scala.Function2%29)                                   | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#reduce%28scala.Function2%29)                               | Aggregate the elements of the dataset using a function (which takes two arguments and returns one). The function should be commutative and associative so that it can be computed correctly in parallel.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [collect()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#collect%28%29)                                                        | No                                                                                                                                                             | Return all the elements of the dataset as an array at the driver program. This is usually useful after a filter or other operation that returns a sufficiently small subset of the data.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [count()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#count%28%29)                                                            | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#count%28%29)                                               | Return the number of elements in the dataset.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [countByValue()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#countByValue%28scala.math.Ordering%29)                           | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByValue%28int, scala.math.Ordering%29)                | Return the count of each unique value in this RDD as a local map of (value, count) pairs.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [first()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#first%28%29)                                                            | No                                                                                                                                                             | Return the first element of the dataset (similar to take(1)).
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [take(n)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#take%28int%29)                                                          | No                                                                                                                                                             | Return an array with the first n elements of the dataset.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [takeSample(withReplacement, num, [seed])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#takeSample%28boolean, int, long%29)    | No                                                                                                                                                             | Return an array with a random sample of num elements of the dataset, with or without replacement, optionally pre-specifying a random number generator seed.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [takeOrdered(n, [ordering])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#takeOrdered%28int, scala.math.Ordering%29)           | No                                                                                                                                                             | Return the first n elements of the RDD using either their natural order or a custom comparator.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [saveAsTextFile(path)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#saveAsTextFile%28java.lang.String%29)                      | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#saveAsTextFiles%28java.lang.String, java.lang.String%29)   | Write the elements of the dataset as a text file (or set of text files) in a given directory in the local filesystem, HDFS or any other Hadoop-supported file system. Spark will call toString on each element to convert it to a line of text in the file.
[SequenceFileRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)      | [saveAsSequenceFile(path) (Java and Scala)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/SequenceFileRDDFunctions.html)                 | No                                                                                                                                                             | Write the elements of the dataset as a Hadoop SequenceFile in a given path in the local filesystem, HDFS or any other Hadoop-supported file system. This is available on RDDs of key-value pairs that implement Hadoop's Writable interface. In Scala, it is also available on types that are implicitly convertible to Writable (Spark includes conversions for basic types like Int, Double, String, etc).
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [saveAsObjectFile(path) (Java and Scala)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#saveAsObjectFile%28java.lang.String%29) | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#saveAsObjectFiles%28java.lang.String, java.lang.String%29) | Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html) | [countByKey()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#countByKey%28%29)                                     | No                                                                                                                                                             | Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                  | [foreach(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#foreach%28scala.Function1%29)                                 | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#foreachRDD%28scala.Function1%29)                           | Run a function on each element of the dataset. This is usually done for side effects such as updating an Accumulator or interacting with external storage systems. Note: modifying variables other than Accumulators outside of the foreach() may result in undefined behavior. See Understanding closures for more details.

## Other Methods

Where                                                                                        | Function                                                                                                             | DStream API                                                                                                                                              | Description
-------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [cache()](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#cache%28%29)                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#cache%28%29)                                         | Don't be afraid to call cache on RDDs to avoid unnecessary recomputation. NOTE: This is the same as persist(MEMORY_ONLY)
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [persist([Storage Level])](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#persist%28%29) | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#persist%28%29)                                       | Persist this RDD with the default storage level
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [unpersist()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#unpersist%28boolean%29)    | No                                                                                                                                                       | Mark the RDD as non-persistent, and remove all blocks for it from memory and disk.
[RDD Interface](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [checkpoint()](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#checkpoint%28%29)          | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#checkpoint%28org.apache.spark.streaming.Duration%29) | Save to a file inside the checkpoint directory and all references to its parent RDDs will be removed.

## Additional Transformation and Actions

Where                                                                                            | Function                                                                                                                                                                                              | Description
------------------------------------------------------------------------------------------------ | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [doubleRDDToDoubleRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/DoubleRDDFunctions.html)                                                                           | Extra functions available on RDDs of Doubles
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [numericRDDToDoubleRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/DoubleRDDFunctions.html)                                                                          | Extra functions available on RDDs of Doubles
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [rddToPairRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)                                                                                     | Extra functions available on RDDs of (key, value) pairs
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [hadoopFile](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopFile%28java.lang.String, java.lang.Class, java.lang.Class, java.lang.Class, int%29)               | Get an RDD for a Hadoop file with an arbitrary InputFormat
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [hadoopRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopRDD%28org.apache.hadoop.mapred.JobConf, java.lang.Class, java.lang.Class, java.lang.Class, int%29) | Get an RDD for a Hadoop file with an arbitrary InputFormat
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [makeRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#makeRDD%28scala.collection.Seq, scala.reflect.ClassTag%29)                                                 | Distribute a local Scala collection to form an RDD
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [parallelize](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#parallelize%28scala.collection.Seq, int, scala.reflect.ClassTag%29)                                    | Distribute a local Scala collection to form an RDD
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [textFile](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#textFile%28java.lang.String, int%29)                                                                      | Read a text file from a file system URI
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [wholeTextFiles](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#wholeTextFiles%28java.lang.String, int%29)                                                          | Read a directory of text files from a file system URI

## Extended RDDs w/ Custom Transformations and Actions

Where        | RDD Name                                                                                             | Description
------------ | ---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
RDD Subclass | [CoGroupedRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/CoGroupedRDD.html) | A RDD that cogroups its parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key.
RDD Subclass | [EdgeRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/graphx/EdgeRDD.html)        | Storing the edges in columnar format on each partition for performance. It may additionally store the vertex attributes associated with each edge to provide the triplet view (GraphX)
RDD Subclass | [JdbcRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/JdbcRDD.html)           | An RDD that executes an SQL query on a JDBC connection and reads results. For usage example, see test case JdbcRDDSuite.
RDD Subclass | [ShuffledRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/ShuffledRDD.html)   | The resulting RDD from a shuffle
RDD Subclass | [VertexRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/graphx/VertexRDD.html)    | Ensures that there is only one entry for each vertex and by pre-indexing the entries for fast, efficient joins. Two VertexRDDs with the same index can be joined efficiently (GraphX)

## RDD Persistence

Storage Level                                                                                                                                 | Meaning
--------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[MEMORY_ONLY (default level)](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)            | Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly each time they're needed.
[MEMORY_AND_DISK](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                        | Store RDD as deserialized Java objects in the JVM. If the RDD does not fit in memory, store the partitions that don't fit on disk, and read them from there when they're needed.
[MEMORY_ONLY_SER](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                        | Store RDD as serialized Java objects (one byte array per partition). This is generally more space-efficient than deserialized objects, especially when using a fast serializer, but more CPU-intensive to read.
[MEMORY_AND_DISK_SER](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                    | Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.
[DISK_ONLY](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                              | Store the RDD partitions only on disk.
[MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html) | Same as the levels above, but replicate each partition on two cluster nodes.

## Shared Data
[Broadcast Variables](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#broadcast%28T, scala.reflect.ClassTag%29) Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

Language | Create, Evaluate
-------- | ------------------------------------------------------------------
Scala    | val broadcastVar = sc.broadcast(Array(1, 2, 3))
         | broadcastVar.value
Java     | Broadcast<int[]> broadcastVar = sc.broadcast(new int[] {1, 2, 3});
         | broadcastVar.value();
Python   | broadcastVar = sc.broadcast([1, 2, 3])
         | broadcastVar.value

[Accumulators](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#accumulator%28T, org.apache.spark.AccumulatorParam%29) Accumulators are variables that are only "added" to through an associative operation and can therefore be efficiently supported in parallel.

Language | Create, Add, Evaluate
-------- | --------------------------------------------------------------------
Scala    | val accum = sc.accumulator(0, My Accumulator)
         | sc.parallelize(Array(1, 2, 3, 4)).foreach(x => accum += x)
         | accum.value
Java     | Accumulator<Integer> accum = sc.accumulator(0);
         | sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -> accum.add(x))
         | accum.value();
Python   | accum = sc.accumulator(0)
         | sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))
         | accum.value

## Streaming only Transformations

Where                                                                                                                               | Function                                                                                                                                                                                                                                                                                                                                              | Description
----------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [window(windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#window%28org.apache.spark.streaming.Duration%29)                                                                                                                                                                  | Return a new DStream which is computed based on windowed batches of the source DStream.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [countByWindow(windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByWindow%28org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration%29)                                                                                                               | Return a sliding window count of elements in the stream.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [reduceByWindow(func, windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#reduceByWindow%28scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration%29)                                                                                      | Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using func. The function should be associative so that it can be computed correctly in parallel.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [reduceByKeyAndWindow(func, windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKeyAndWindow%28scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int%29)                                            | When called on a DStream of (K, V) pairs, returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function func over batches in a sliding window. Note: By default, this uses Spark's default number of parallel tasks (2 for local mode, and in cluster mode the number is determined by the config property spark.default.parallelism) to do the grouping. You can pass an optional numTasks argument to set a different number of tasks.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [reduceByKeyAndWindow(func, invFunc, windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKeyAndWindow%28scala.Function2, scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int, scala.Function1%29) | A more efficient version of the above reduceByKeyAndWindow() where the reduce value of each window is calculated incrementally using the reduce values of the previous window. This is done by reducing the new data that enters the sliding window, and “inverse reducing” the old data that leaves the window. An example would be that of “adding” and “subtracting” counts of keys as the window slides. However, it is applicable only to “invertible reduce functions”, that is, those reduce functions which have a corresponding “inverse reduce” function (taken as parameter invFunc). Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument. Note that checkpointing must be enabled for using this operation.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [countByValueAndWindow(windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByValueAndWindow%28org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int, scala.math.Ordering%29)                                                         | When called on a DStream of (K, V) pairs, returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window. Like in reduceByKeyAndWindow, the number of reduce tasks is configurable through an optional argument.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [transform(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#transform%28scala.Function1, scala.reflect.ClassTag%29)                                                                                                                                                                           | The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a Dstream.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [updateStateByKey(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#updateStateByKey%28scala.Function2, scala.reflect.ClassTag%29)                                                                                                                                                | The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information. To use this, you will have to do two steps.

## Mllib Reference

Topic                                                                                                      | Description
---------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------------------------------------------------
[Data types](http://spark.apache.org/docs/latest/mllib-data-types.html)                                    | Vectors, points, matrices
[Basic Statistics](http://spark.apache.org/docs/latest/mllib-statistics.html)                              | Summary, correlations, sampling, testing and random data
[Classification and regression](http://spark.apache.org/docs/latest/mllib-classification-regression.html)  | Includes SVMs, decision trees, naïve Bayes, etc..
[Collaborative filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)          | Commonly used for recommender systems
[Clustering](http://spark.apache.org/docs/latest/mllib-clustering.html)                                    | Clustering is an unsupervised learning approach
[Dimensionality reduction](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)        | Dimensionality reduction is the process of reducing the number of variables under consideration
[Feature extraction and transformation](http://spark.apache.org/docs/latest/mllib-feature-extraction.html) | Used in selecting a subset of relevant features (variables, predictors) for use in model construction
[Frequent pattern mining](http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html)          | Mining frequent items, itemsets, subsequences, or other substructures is usually among the first steps to analyze a large-scale dataset.
[Optimization](http://spark.apache.org/docs/latest/mllib-optimization.html)                                | Different optimization methods can have different convergence guarantees depending on the properties of the objective function
[PMML model export](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)           | MLlib supports model export to Predictive Model Markup Language
