# Apache Spark Cheat Sheet
## Transformations (return new RDDs – Lazy)

Where                                                                                                               | Function                                                                                                                                                                                                                         | DStream API                                                                                                                                                                                       | Description
------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [map(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#map%28scala.Function1, scala.reflect.ClassTag%29)                                                                                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#map%28scala.Function1, scala.reflect.ClassTag%29)                                             | Return a new distributed dataset formed by passing each element of the source through a function.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [filter(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#filter%28scala.Function1%29)                                                                                                      | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#filter%28scala.Function1%29)                                                                  | Return a new dataset formed by selecting those elements of the source on which function returns true.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [filterByRange(lower, upper)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html#filterByRange%28K, K%29)                                                                               | No                                                                                                                                                                                                | Returns an RDD containing only the elements in the the inclusive range lower to upper.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [flatMap(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#flatMap%28scala.Function1, scala.reflect.ClassTag%29)                                                                            | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#flatMap%28scala.Function1, scala.reflect.ClassTag%29)                                         | Similar to map, but each input item can be mapped to 0 or more output items (so function should return a Seq rather than a single item).
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [mapPartitions(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#mapPartitions%28scala.Function1, boolean, scala.reflect.ClassTag%29)                                                       | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#mapPartitions%28scala.Function1, boolean, scala.reflect.ClassTag%29)                          | Similar to map, but runs separately on each partition of the RDD.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [mapPartitionsWithIndex(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#mapPartitionsWithIndex%28scala.Function2, boolean, scala.reflect.ClassTag%29)                                     | No                                                                                                                                                                                                | Similar to mapPartitions, but also provides function with an integer value representing the index of the partition.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [sample(withReplacement, fraction, seed)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#sample%28boolean, double, long%29)                                                                         | No                                                                                                                                                                                                | Sample a fraction of the data, with or without replacement, using a given random number generator seed.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [union(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#union%28org.apache.spark.rdd.RDD%29)                                                                                           | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#union%28org.apache.spark.streaming.dstream.DStream%29)                                        | Return a new dataset that contains the union of the elements in the datasets.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [intersection(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#intersection%28org.apache.spark.rdd.RDD%29)                                                                             | No                                                                                                                                                                                                | Return a new RDD that contains the intersection of elements in the datasets.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [distinct([numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#distinct%28%29)                                                                                                               | No                                                                                                                                                                                                | Return a new dataset that contains the distinct elements of the source dataset.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [groupByKey([numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#groupByKey%28%29)                                                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#groupByKey%28%29)                                                                | Returns a dataset of (K, Iterable&lt;V&gt;) pairs. Use reduceByKey or aggregateByKey to perform an aggregation (such as a sum or average).
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [reduceByKey(function, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#reduceByKey%28scala.Function2%29)                                                                   | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKey%28scala.Function2%29)                                                | Returns a dataset of (K, V) pairs where the values for each key are aggregated using the given reduce function.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                    | [aggregateByKey(zeroValue)(seqOp, combOp, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#aggregateByKey%28U, scala.Function2, scala.Function2, scala.reflect.ClassTag%29) | No                                                                                                                                                                                                | Returns a dataset of (K, U) pairs where the values for each key are aggregated using the given combine functions and a neutral "zero" value. Allows an aggregated value type that is different than the input value type.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [sortByKey([ascending], [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html)                                                                                                | No                                                                                                                                                                                                | Returns a dataset of (K, V) pairs sorted by keys in ascending or descending order, as specified in the boolean ascending argument.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                    | [join(otherDataset, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#join%28org.apache.spark.rdd.RDD%29)                                                                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#join%28org.apache.spark.streaming.dstream.DStream, scala.reflect.ClassTag%29)    | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (V, W)) pairs with all pairs of elements for each key. Outer joins are supported through leftOuterJoin, rightOuterJoin, and fullOuterJoin.
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)       | [cogroup(otherDataset, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#cogroup%28org.apache.spark.rdd.RDD%29)                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#cogroup%28org.apache.spark.streaming.dstream.DStream, scala.reflect.ClassTag%29) | When called on datasets of type (K, V) and (K, W), returns a dataset of (K, (Iterable&lt;V&gt;, Iterable&lt;W&gt;)) tuples.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [cartesian(otherDataset)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#cartesian%28org.apache.spark.rdd.RDD, scala.reflect.ClassTag%29)                                                           | No                                                                                                                                                                                                | When called on datasets of types T and U, returns a dataset of (T, U) pairs (all pairs of elements).
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [pipe(command, [envVars])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#pipe%28scala.collection.Seq, scala.collection.Map, scala.Function1, scala.Function2, boolean%29)                          | No                                                                                                                                                                                                | Pipe each partition of the RDD through a shell command, e.g. a Perl or bash script.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [coalesce(numPartitions)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#coalesce%28int, boolean, scala.math.Ordering%29)                                                                           | No                                                                                                                                                                                                | Decrease the number of partitions in the RDD to numPartitions. Useful for running operations more efficiently after filtering down a large dataset.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                                  | [repartition(numPartitions)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#repartition%28int, scala.math.Ordering%29)                                                                              | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#repartition%28int%29)                                                                         | Reshuffle the data in the RDD randomly to create either more or fewer partitions and balance it across them. This always shuffles all data over the network.
[OrderedRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html) | [repartitionAndSortWithinPartitions(partitioner)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/OrderedRDDFunctions.html)                                                                                   | No                                                                                                                                                                                                | Repartition the RDD according to the given partitioner and, within each resulting partition, sort records by their keys. More efficient than calling repartition and then sorting.

## Actions (return values – NOT Lazy)

Where                                                                                                         | Function                                                                                                                                                      | DStream API                                                                                                                                                    | Description
------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [reduce(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#reduce%28scala.Function2%29)                                   | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#reduce%28scala.Function2%29)                               | Aggregate the elements of the dataset using a function (which takes two arguments and returns one).
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [collect()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#collect%28%29)                                                        | No                                                                                                                                                             | Return all the elements of the dataset as an array at the driver program. Best used on sufficiently small subsets of data.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [count()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#count%28%29)                                                            | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#count%28%29)                                               | Return the number of elements in the dataset.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [countByValue()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#countByValue%28scala.math.Ordering%29)                           | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByValue%28int, scala.math.Ordering%29)                | Return the count of each unique value in this RDD as a local map of (value, count) pairs.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [first()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#first%28%29)                                                            | No                                                                                                                                                             | Return the first element of the dataset (similar to take(1)).
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [take(n)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#take%28int%29)                                                          | No                                                                                                                                                             | Return an array with the first n elements of the dataset.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [takeSample(withReplacement, num, [seed])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#takeSample%28boolean, int, long%29)    | No                                                                                                                                                             | Return an array with a random sample of num elements of the dataset.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [takeOrdered(n, [ordering])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#takeOrdered%28int, scala.math.Ordering%29)           | No                                                                                                                                                             | Return the first n elements of the RDD using either their natural order or a custom comparator.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [saveAsTextFile(path)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#saveAsTextFile%28java.lang.String%29)                      | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#saveAsTextFiles%28java.lang.String, java.lang.String%29)   | Write the elements of the dataset as a text. Spark will call toString on each element to convert it to a line of text in the file.
[SequenceFileRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)      | [saveAsSequenceFile(path) (Java and Scala)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/SequenceFileRDDFunctions.html)                 | No                                                                                                                                                             | Write the elements of the dataset as a Hadoop SequenceFile in a given path. For RDDs of key-value pairs that use Hadoop's Writable interface.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [saveAsObjectFile(path) (Java and Scala)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#saveAsObjectFile%28java.lang.String%29) | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#saveAsObjectFiles%28java.lang.String, java.lang.String%29) | Write the elements of the dataset in a simple format using Java serialization, which can then be loaded using SparkContext.objectFile().
[PairRDD Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html) | [countByKey()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html#countByKey%28%29)                                     | No                                                                                                                                                             | Only available on RDDs of type (K, V). Returns a hashmap of (K, Int) pairs with the count of each key.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)                            | [foreach(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#foreach%28scala.Function1%29)                                 | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#foreachRDD%28scala.Function1%29)                           | Run a function on each element of the dataset. This is usually done for side effects such as updating an Accumulator.

## Persistence Methods

Where                                                                              | Function                                                                                                             | DStream API                                                                                                                                              | Description
---------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------------------------
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [cache()](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#cache%28%29)                    | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#cache%28%29)                                         | Don't be afraid to call cache on RDDs to avoid unnecessary recomputation. NOTE: This is the same as persist(MEMORY_ONLY).
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [persist([Storage Level])](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#persist%28%29) | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#persist%28%29)                                       | Persist this RDD with the default storage level.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [unpersist()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#unpersist%28boolean%29)    | No                                                                                                                                                       | Mark the RDD as non-persistent, and remove its blocks from memory and disk.
[RDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html) | [checkpoint()](http://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html#checkpoint%28%29)          | [Yes](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#checkpoint%28org.apache.spark.streaming.Duration%29) | Save to a file inside the checkpoint directory and all references to its parent RDDs will be removed.

## Additional Transformation and Actions

Where                                                                                            | Function                                                                                                                                                                                                | Description
------------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [doubleRDDToDoubleRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/DoubleRDDFunctions.html)                                                                             | Extra functions available on RDDs of Doubles
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [numericRDDToDoubleRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/DoubleRDDFunctions.html)                                                                            | Extra functions available on RDDs of Doubles
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [rddToPairRDDFunctions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/PairRDDFunctions.html)                                                                                       | Extra functions available on RDDs of (key, value) pairs
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [hadoopFile()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopFile%28java.lang.String, java.lang.Class, java.lang.Class, java.lang.Class, int%29)               | Get an RDD for a Hadoop file with an arbitrary InputFormat
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [hadoopRDD()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#hadoopRDD%28org.apache.hadoop.mapred.JobConf, java.lang.Class, java.lang.Class, java.lang.Class, int%29) | Get an RDD for a Hadoop file with an arbitrary InputFormat
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [makeRDD()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#makeRDD%28scala.collection.Seq, scala.reflect.ClassTag%29)                                                 | Distribute a local Scala collection to form an RDD
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [parallelize()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#parallelize%28scala.collection.Seq, int, scala.reflect.ClassTag%29)                                    | Distribute a local Scala collection to form an RDD
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [textFile()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#textFile%28java.lang.String, int%29)                                                                      | Read a text file from a file system URI
[SparkContext](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html) | [wholeTextFiles()](https://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#wholeTextFiles%28java.lang.String, int%29)                                                          | Read a directory of text files from a file system URI

## Extended RDDs w/ Custom Transformations and Actions

RDD Name                                                                                             | Description
---------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------
[CoGroupedRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/CoGroupedRDD.html) | A RDD that cogroups its parents. For each key k in parent RDDs, the resulting RDD contains a tuple with the list of values for that key.
[EdgeRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/graphx/EdgeRDD.html)        | Storing the edges in columnar format on each partition for performance. It may additionally store the vertex attributes associated with each edge.
[JdbcRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/JdbcRDD.html)           | An RDD that executes an SQL query on a JDBC connection and reads results. For usage example, see test case JdbcRDDSuite.
[ShuffledRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/ShuffledRDD.html)   | The resulting RDD from a shuffle.
[VertexRDD](https://spark.apache.org/docs/latest/api/java/org/apache/spark/graphx/VertexRDD.html)    | Ensures that there is only one entry for each vertex and by pre-indexing the entries for fast, efficient joins.

## Streaming Transformations

Where                                                                                                                               | Function                                                                                                                                                                                                                                                                                                                                                  | Description
----------------------------------------------------------------------------------------------------------------------------------- | --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [window(windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#window%28org.apache.spark.streaming.Duration%29)                                                                                                                                                                      | Return a new DStream which is computed based on windowed batches of the source DStream.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [countByWindow(windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByWindow%28org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration%29)                                                                                                                   | Return a sliding window count of elements in the stream.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [reduceByWindow(function, windowLength, slideInterval)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#reduceByWindow%28scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration%29)                                                                                      | Return a new single-element stream, created by aggregating elements in the stream over a sliding interval using function.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [reduceByKeyAndWindow(function, windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKeyAndWindow%28scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int%29)                                            | Returns a new DStream of (K, V) pairs where the values for each key are aggregated using the given reduce function over batches in a sliding window.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [reduceByKeyAndWindow(function, invFunc, windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#reduceByKeyAndWindow%28scala.Function2, scala.Function2, org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int, scala.Function1%29) | A more efficient version of the above reduceByKeyAndWindow(). Only applicable to those reduce functions which have a corresponding "inverse reduce" function. Checkpointing must be enabled for using this operation.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [countByValueAndWindow(windowLength, slideInterval, [numTasks])](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#countByValueAndWindow%28org.apache.spark.streaming.Duration, org.apache.spark.streaming.Duration, int, scala.math.Ordering%29)                                                             | Returns a new DStream of (K, Long) pairs where the value of each key is its frequency within a sliding window.
[DStream](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html)                            | [transform(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/DStream.html#transform%28scala.Function1, scala.reflect.ClassTag%29)                                                                                                                                                                               | The transform operation (along with its variations like transformWith) allows arbitrary RDD-to-RDD functions to be applied on a Dstream.
[PairDStream Functions](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html) | [updateStateByKey(function)](https://spark.apache.org/docs/latest/api/java/org/apache/spark/streaming/dstream/PairDStreamFunctions.html#updateStateByKey%28scala.Function2, scala.reflect.ClassTag%29)                                                                                                                                                    | The updateStateByKey operation allows you to maintain arbitrary state while continuously updating it with new information.

## RDD Persistence

Storage Level                                                                                                                                   | Meaning
----------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------
[MEMORY_ONLY (default level)](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)              | Store RDD as deserialized Java objects. If the RDD does not fit in memory, some partitions will not be cached and will be recomputed on the fly when needed.
[MEMORY_AND_DISK](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                          | Store RDD as deserialized Java objects. If the RDD does not fit in memory, store the partitions that don't fit on disk, and load them when they're needed.
[MEMORY_ONLY_SER](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                          | Store RDD as serialized Java objects. Generally more space-efficient than deserialized objects, but more CPU-intensive to read.
[MEMORY_AND_DISK_SER](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                      | Similar to MEMORY_ONLY_SER, but spill partitions that don't fit in memory to disk instead of recomputing them on the fly each time they're needed.
[DISK_ONLY](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html)                                | Store the RDD partitions only on disk.
[MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc...](https://spark.apache.org/docs/latest/api/java/index.html?org/apache/spark/storage/StorageLevel.html) | Same as the levels above, but replicate each partition on two cluster nodes.

## Shared Data
[Broadcast Variables](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#broadcast%28T, scala.reflect.ClassTag%29) Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks.

Language | Create, Evaluate
-------- | ------------------------------------------------------------------------
Scala    | val broadcastVar = sc.broadcast(Array(1, 2, 3))
         | broadcastVar.value
Java     | Broadcast&lt;int[]&gt; broadcastVar = sc.broadcast(new int[] {1, 2, 3});
         | broadcastVar.value();
Python   | broadcastVar = sc.broadcast([1, 2, 3])
         | broadcastVar.value

[Accumulators](http://spark.apache.org/docs/latest/api/java/org/apache/spark/SparkContext.html#accumulator%28T, org.apache.spark.AccumulatorParam%29) Accumulators are variables that are only "added" to through an associative operation and can therefore be efficiently supported in parallel.

Language | Create, Add, Evaluate
-------- | -----------------------------------------------------------------------
Scala    | val accum = sc.accumulator(0, My Accumulator)
         | sc.parallelize(Array(1, 2, 3, 4)).foreach(x =&gt; accum += x)
         | accum.value
Java     | Accumulator&lt;Integer&gt; accum = sc.accumulator(0);
         | sc.parallelize(Arrays.asList(1, 2, 3, 4)).foreach(x -&gt; accum.add(x))
         | accum.value();
Python   | accum = sc.accumulator(0)

## MLlib Reference

Topic                                                                                                      | Description
---------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------
[Data types](http://spark.apache.org/docs/latest/mllib-data-types.html)                                    | Vectors, points, matrices.
[Basic Statistics](http://spark.apache.org/docs/latest/mllib-statistics.html)                              | Summary, correlations, sampling, testing and random data.
[Classification and regression](http://spark.apache.org/docs/latest/mllib-classification-regression.html)  | Includes SVMs, decision trees, naïve Bayes, etc...
[Collaborative filtering](http://spark.apache.org/docs/latest/mllib-collaborative-filtering.html)          | Commonly used for recommender systems.
[Clustering](http://spark.apache.org/docs/latest/mllib-clustering.html)                                    | Clustering is an unsupervised learning approach.
[Dimensionality reduction](http://spark.apache.org/docs/latest/mllib-dimensionality-reduction.html)        | Dimensionality reduction is the process of reducing the number of variables under consideration.
[Feature extraction and transformation](http://spark.apache.org/docs/latest/mllib-feature-extraction.html) | Used in selecting a subset of relevant features (variables, predictors) for use in model construction.
[Frequent pattern mining](http://spark.apache.org/docs/latest/mllib-frequent-pattern-mining.html)          | Mining is usually among the first steps to analyze a large-scale dataset.
[Optimization](http://spark.apache.org/docs/latest/mllib-optimization.html)                                | Different optimization methods can have different convergence guarantees.
[PMML model export](https://spark.apache.org/docs/latest/api/java/org/apache/spark/rdd/RDD.html)           | MLlib supports model export to Predictive Model Markup Language.

## Other References
- [Launching Jobs](https://spark.apache.org/docs/latest/submitting-applications.html)
- [SQL and DataFrames Programming Guide](http://spark.apache.org/docs/latest/sql-programming-guide.html)
- [GraphX Programming Guide](http://spark.apache.org/docs/latest/graphx-programming-guide.html)
- [SparkR Programming Guide](http://spark.apache.org/docs/latest/sparkr.html)
